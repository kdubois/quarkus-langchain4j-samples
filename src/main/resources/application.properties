#quarkus.langchain4j.openai.api-key=demo

%dev.quarkus.mailer.mock=false

quarkus.langchain4j.easy-rag.path=src/main/resources/catalog

booking.daystostart=1
booking.daystoend=5

# With Ollama and llama 3.1
#quarkus.langchain4j.openai.base-url=http://localhost:11434/v1
# Configure openai server to use a specific model
#quarkus.langchain4j.openai.chat-model.model-name=llama3.1
#quarkus.langchain4j.openai.embedding-model.model-name=llama3.1

# With Podman Desktop & an inference server on port 35000
#quarkus.langchain4j.openai.base-url=http://localhost:35000/v1

# With ChatGPT
# Configure openai server to use a specific model
quarkus.langchain4j.openai.chat-model.model-name=gpt-4o

# Choose a low temperature to minimize hallucination
quarkus.langchain4j.openai.chat-model.temperature=0
# Set timeout to 3 minutes (local LLM can be quite slow)
quarkus.langchain4j.openai.timeout=180s
# Enable logging of both requests and responses
quarkus.langchain4j.openai.log-requests=true
quarkus.langchain4j.openai.log-responses=true

quarkus.langchain4j.openai.image-generation.chat-model.model-name=dall-e-3